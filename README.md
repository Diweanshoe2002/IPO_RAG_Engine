# IPO RAG Research Assistant

A production-grade Retrieval-Augmented Generation (RAG) system for analyzing Indian IPO documents including DRHP (Draft Red Herring Prospectus) and RHP (Red Herring Prospectus) with zero-hallucination guarantees.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Streamlit Interface                           â”‚
â”‚                    (User Query Input)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   IPO Registry       â”‚
                    â”‚   (Company â†’ Doc)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FAISS Retrieval  â”‚      â”‚  Document Ingestionâ”‚
    â”‚ (Isolated Index) â”‚      â”‚  (One-time)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                          â”‚
              â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚  Docling Parser    â”‚
              â”‚                â”‚  - OCR             â”‚
              â”‚                â”‚  - Layout Analysis â”‚
              â”‚                â”‚  - Table Extract   â”‚
              â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                          â”‚
              â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚  HybridChunker     â”‚
              â”‚                â”‚  - Structure-aware â”‚
              â”‚                â”‚  - Token-safe      â”‚
              â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                          â”‚
              â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚  Embeddings        â”‚
              â”‚                â”‚  â†’ FAISS Index     â”‚
              â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Retrieved Chunks â”‚
    â”‚ (Top-K Context)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ DSPy Reasoning   â”‚
    â”‚ (Chain-of-Thought)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Structured Answerâ”‚
    â”‚ (With Citations) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features

### 1. **Production-Grade Document Processing**
- **Docling Integration**: Advanced OCR and layout-aware parsing
- **HybridChunker**: Structure-aware, token-safe chunking
- **Table Preservation**: Multi-page table cell matching
- **Section Context**: Maintains legal document hierarchy

### 2. **Zero-Hallucination Architecture**
- **Isolated Vector Stores**: Per-company FAISS indexes
- **Deterministic Retrieval**: Similarity-based Top-K retrieval
- **Source-Grounded Answers**: All responses cite IPO documents
- **No Cross-Contamination**: Company data never mixed

### 3. **Intelligent Reasoning**
- **DSPy Chain-of-Thought**: Programmatic reasoning modules
- **Multi-Section Analysis**: Connects information across document sections
- **Financial Table Understanding**: Extracts structured financial data
- **Legal Context Awareness**: Understands IPO document structure

### 4. **Scalable Design**
- **One-Time Ingestion**: Process documents once, query forever
- **Persistent Storage**: FAISS indexes cached on disk
- **Fast Retrieval**: Optimized vector similarity search
- **Modular Architecture**: Easy to extend and maintain

## Project Structure

```
ipo-rag-assistant/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py              # Configuration management
â”‚   â”‚   â””â”€â”€ llm_config.py            # LLM & embedding setup
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ docling_parser.py        # Docling PDF processor
â”‚   â”‚   â”œâ”€â”€ hybrid_chunker.py        # Structure-aware chunking
â”‚   â”‚   â””â”€â”€ pipeline.py              # Complete ingestion pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ faiss_manager.py         # FAISS index management
â”‚   â”‚   â””â”€â”€ embedding_generator.py   # Embedding creation
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retriever.py             # Document retrieval logic
â”‚   â”‚   â””â”€â”€ ipo_registry.py          # Company-to-document mapping
â”‚   â”‚
â”‚   â”œâ”€â”€ reasoning/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dspy_chain.py            # DSPy reasoning modules
â”‚   â”‚   â””â”€â”€ answer_formatter.py      # Response structuring
â”‚   â”‚
â”‚   â””â”€â”€ streamlit_app.py             # Main Streamlit interface
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # IPO PDF documents
â”‚   â”‚   â”œâ”€â”€ DRHP/
â”‚   â”‚   â””â”€â”€ RHP/
â”‚   â”œâ”€â”€ vectorstore/                 # Persistent FAISS indexes
â”‚   â””â”€â”€ registry/                    # Company metadata
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_documents.py          # Batch document ingestion
â”‚   â”œâ”€â”€ rebuild_index.py             # Index rebuilding utility
â”‚   â””â”€â”€ validate_setup.py            # Environment validation
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ .env.example                 # Environment template
â”‚   â””â”€â”€ companies.yaml               # IPO company registry
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md              # Detailed architecture
â”‚   â”œâ”€â”€ docling_guide.md             # Docling integration guide
â”‚   â”œâ”€â”€ dspy_reasoning.md            # DSPy reasoning explanation
â”‚   â””â”€â”€ api_reference.md             # API documentation
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â””â”€â”€ test_reasoning.py
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .env                             # Environment variables (gitignored)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Quick Start

### Prerequisites

- Python 3.9+
- Cerebras API key (for LLM inference)
- Sufficient disk space for PDF storage and FAISS indexes

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ipo-rag-assistant
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**
   ```bash
   cp config/.env.example .env
   # Edit .env with your API keys
   ```

5. **Validate setup**
   ```bash
   python scripts/validate_setup.py
   ```

### Configuration

Edit `.env` file with your credentials:

```env
# Cerebras API
CEREBRAS_API_KEY=your-cerebras-api-key

# Embedding Configuration
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384

# LLM Configuration
LLM_MODEL=llama3.1-8b
LLM_TEMPERATURE=0.1
MAX_TOKENS=2048

# Retrieval Configuration
TOP_K_RETRIEVAL=5
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# Storage Paths
RAW_DATA_DIR=./data/raw
VECTORSTORE_DIR=./data/vectorstore
REGISTRY_PATH=./config/companies.yaml
```

## Usage

### 1. Ingest IPO Documents

Place your IPO PDF documents in `data/raw/` directory:

```bash
data/raw/
â”œâ”€â”€ DRHP/
â”‚   â”œâ”€â”€ company_a_drhp.pdf
â”‚   â””â”€â”€ company_b_drhp.pdf
â””â”€â”€ RHP/
    â””â”€â”€ company_a_rhp.pdf
```

Run the ingestion pipeline:

```bash
python scripts/ingest_documents.py --input-dir data/raw/DRHP
```

The system will:
- Parse PDFs using Docling (OCR + layout analysis)
- Generate structure-aware chunks with HybridChunker
- Create embeddings
- Build and persist FAISS indexes

### 2. Launch Streamlit Interface

```bash
streamlit run app/streamlit_app.py
```

Access the interface at `http://localhost:8501`

### 3. Query IPO Documents

**Example Queries:**

- "What is the company's revenue growth over the last 3 years?"
- "Who are the key promoters and their shareholding patterns?"
- "What are the risk factors mentioned in the IPO?"
- "Explain the use of proceeds from this IPO"
- "Compare the P/E ratio with industry peers"

### 4. Programmatic Usage

```python
from app.rag.retriever import IPORetriever
from app.reasoning.dspy_chain import IPOAnalystChain

# Initialize retriever
retriever = IPORetriever(company="TechCorp")

# Retrieve relevant chunks
chunks = retriever.retrieve("What is the company's revenue?", top_k=5)

# Generate answer with DSPy reasoning
analyst = IPOAnalystChain()
answer = analyst.analyze(question="What is the company's revenue?", context=chunks)

print(answer.response)
print(answer.reasoning_steps)
```

## ğŸ”§ Advanced Features

### Document Processing Pipeline

#### Docling: Why It Matters

IPO documents are **not normal text**:
- âœ… Tables span multiple pages
- âœ… Headers define legal context
- âœ… OCR quality varies across scans
- âœ… Complex layouts with mixed content

**Docling Responsibilities:**
- OCR extraction from scanned PDFs
- Layout-aware parsing (columns, headers, footers)
- Table cell matching across pages
- Section hierarchy preservation
- Figure and diagram detection

#### HybridChunker Advantages

Traditional chunking methods fail on IPO documents. HybridChunker provides:

- **Size-aware**: Respects token limits for embedding models
- **Structure-aware**: Preserves section and table boundaries
- **Context preservation**: Maintains parent-child relationships
- **Reduced over-chunking**: Minimizes information fragmentation
- **Improved retrieval**: Better semantic unit representation

**Chunking Strategy:**

```python
# Example: Financial table chunking
Table: "Revenue Breakdown"
â”œâ”€ Chunk 1: Table header + Year 2021-2022 data
â”œâ”€ Chunk 2: Table header + Year 2022-2023 data
â””â”€ Chunk 3: Table header + Year 2023-2024 data

# Each chunk includes context for standalone understanding
```

### RAG Layer (FAISS)

**Design Principles:**

1. **Isolated Indexes**: Each IPO has its own FAISS index
   - No cross-company data leakage
   - Deterministic retrieval
   - Independent updates

2. **Persistent Storage**: Indexes cached on disk
   - Fast startup times
   - No re-ingestion needed
   - Version control friendly

3. **Similarity-Based Retrieval**: Top-K semantic search
   - Cosine similarity metrics
   - Configurable K values
   - Relevance scoring

4. **Metadata Enrichment**: Each chunk includes:
   - Source document
   - Page numbers
   - Section context
   - Table references

### DSPy: Chain-of-Thought Reasoning

DSPy operates **after retrieval**, never before.

**Why DSPy?**

- âœ… Explicit reasoning modules (not prompt engineering)
- âœ… Chain-of-Thought enforced programmatically
- âœ… Separation of retrieval and reasoning
- âœ… Reproducible outputs
- âœ… Easy to test and validate

**DSPy Flow:**

```
Input:
â”œâ”€ Retrieved IPO chunks (context)
â””â”€ User question

â†“

DSPy Chain-of-Thought:
â”œâ”€ Step 1: Identify relevant information
â”œâ”€ Step 2: Extract key financial metrics
â”œâ”€ Step 3: Analyze trends and patterns
â”œâ”€ Step 4: Formulate answer
â””â”€ Step 5: Generate citations

â†“

Output:
â”œâ”€ Reasoning steps (visible to user)
â”œâ”€ Structured analyst answer
â””â”€ Source citations
```

**Example DSPy Signature:**

```python
class IPOAnalysis(dspy.Signature):
    """Analyze IPO documents with chain-of-thought reasoning"""
    
    question: str = dspy.InputField(desc="User question")
    context: str = dspy.InputField(desc="Retrieved IPO chunks")
    
    reasoning: str = dspy.OutputField(desc="Step-by-step analysis")
    answer: str = dspy.OutputField(desc="Final structured answer")
    sources: List[str] = dspy.OutputField(desc="Source citations")
```

## Workflows

### Workflow 1: Financial Analysis

```
User: "What is the company's EBITDA margin trend?"

System:
1. Retrieves financial tables from last 3 years
2. DSPy identifies EBITDA and revenue figures
3. Calculates margins: 2021: 22%, 2022: 25%, 2023: 28%
4. Analyzes trend: "Improving margin, +6% over 3 years"
5. Cites: Pages 45-47, Financial Statements section
```

### Workflow 2: Risk Assessment

```
User: "What are the top 3 business risks?"

System:
1. Retrieves "Risk Factors" section
2. DSPy categorizes risks by severity
3. Identifies: Regulatory, Competition, Technology
4. Ranks by frequency and prominence
5. Cites: Pages 12-18, Risk Factors section
```

### Workflow 3: Competitive Positioning

```
User: "How does the company compare to competitors?"

System:
1. Retrieves market position and peer data
2. DSPy extracts competitor names and metrics
3. Compares market share, growth rates, margins
4. Analyzes: "Top 3 player, 15% market share, fastest growth"
5. Cites: Pages 8-10, Industry Overview section
```

# ğŸ—ºï¸ Roadmap

- [ ] Multi-language support (Hindi, regional)
- [ ] Comparative analysis across multiple IPOs
- [ ] Financial model generation
- [ ] Automated red flag detection
- [ ] Excel export of extracted data
- [ ] REST API for programmatic access
- [ ] Real-time IPO document monitoring

